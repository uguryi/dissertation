{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import argparse\n",
    "import ast\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine sessions to be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_repo = os.path.join(\"path/to/repo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = pd.read_csv(os.path.join(path_to_repo, \n",
    "                                \"data\", \n",
    "                                \"processed-data\", \n",
    "                                \"experiment-logs.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "end_idx = 0\n",
    "for index, row in logs.iterrows():\n",
    "    lst = list(row)\n",
    "    if all(item != item for item in lst):\n",
    "        end_idx = index\n",
    "        break\n",
    "        \n",
    "logs = logs[:end_idx]\n",
    "logs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lst = list(logs[\"Date\"])\n",
    "for i in range(1, len(lst)):\n",
    "    if lst[i] != lst[i]:\n",
    "        lst[i] = lst[i-1]\n",
    "logs[\"DateFull\"] = lst\n",
    "logs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logs = logs[logs['Use?'] == 'Yes']\n",
    "\n",
    "def get_filenames(row):\n",
    "    date = row[\"DateFull\"]\n",
    "    run = row[\"Run number\"]\n",
    "    cond = \"-\".join(row[\"Condition\"].split(\"_\"))\n",
    "    file_name = \"-\".join([\"experiment\", date, run, cond]) + \".csv\"\n",
    "    return file_name\n",
    "    \n",
    "logs[\"file_name\"] = logs.apply(get_filenames, axis = 1)\n",
    "logs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_filenames = list(logs[\"file_name\"])\n",
    "\n",
    "print(len(lst_filenames))\n",
    "lst_filenames[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (i) Convert each raw part into rectangular format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeRectangular(raw_data):\n",
    "    numTotalEvents = len(raw_data.data_name.values)\n",
    "    numUniqEvents = len(set(raw_data.data_name.values))\n",
    "    rect_data = pd.DataFrame(columns = raw_data.data_name.values[:numUniqEvents])\n",
    "    start = 0\n",
    "    end = numUniqEvents\n",
    "    while start < numTotalEvents:\n",
    "        cur = pd.DataFrame([list(raw_data.data_value[start:end])], \n",
    "                           columns = raw_data.data_name.values[start:end])\n",
    "        # added sort = True to the next line after seeing warning message\n",
    "        rect_data = rect_data.append(cur, ignore_index = True, sort = True)\n",
    "        start = end\n",
    "        end = end + numUniqEvents\n",
    "    if 'round' in rect_data.columns:\n",
    "        rect_data = rect_data[(rect_data['round'] != '0') | (rect_data['version'] == '2')]\n",
    "    return rect_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (ii) Restrict dataset to tutorial clicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_tutorial(row):\n",
    "    return \"Tutorial\" in row.event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_path = \"in/path\"\n",
    "out_path = os.path.join(path_to_repo, \"data\", \"processed-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in lst_filenames:\n",
    "    \n",
    "    # Read raw csv data\n",
    "    df_raw = pd.read_csv(os.path.join(in_path, file))\n",
    "\n",
    "    # Store date and hour\n",
    "    date = re.search('(.*?)T', df_raw['event_date'][0]).group(1)\n",
    "    hour = re.search('T(.*?)Z', df_raw['event_date'][0]).group(1)\n",
    "\n",
    "    # Remove sensitive and unnecessary rows\n",
    "    remove = [\"acceptLanguage\", \"acceptEncoding\", \"userAgent\", \"host\", \"requestURI\", \"referer\", \"connection\"]\n",
    "    df_raw = df_raw[~df_raw.data_name.isin(remove)]\n",
    "\n",
    "    # Replace pid with anonymous identifiers\n",
    "    pattern = re.compile(\"[A-Z0-9]{30}\")\n",
    "    pidsDic = {}\n",
    "    idx = 1\n",
    "    for index, row in df_raw.iterrows():\n",
    "        if pattern.match(str(row.data_value)) and row.data_value not in pidsDic:\n",
    "            pidsDic[str(row.data_value)] = \"p\" + str(idx)\n",
    "            idx += 1\n",
    "    for index, row in df_raw.iterrows():\n",
    "        if pattern.findall(str(row.data_value)):\n",
    "            for match in pattern.findall(str(row.data_value)):\n",
    "                df_raw.loc[index, 'data_value'] = str(row.data_value).replace(match, pidsDic[match])\n",
    "        if pattern.findall(str(row.data_name)):\n",
    "            for match in pattern.findall(str(row.data_name)):\n",
    "                df_raw.loc[index, 'data_name'] = str(row.data_name).replace(match, pidsDic[match])\n",
    "\n",
    "    # Separate data into its different parts\n",
    "    tutorials = df_raw[df_raw.apply(contains_tutorial, axis = 1)]\n",
    "    tutorials = tutorials.pivot(index = 'data_value', \n",
    "                                columns = 'event', \n",
    "                                values = 'event_date').reset_index()\n",
    "    tutorials = tutorials.rename(columns = {\"data_value\": \"pid\"})\n",
    "    if 'Tutorial300' in tutorials.columns:\n",
    "        tutorials['Tutorial301'] = np.nan\n",
    "    else:\n",
    "        tutorials['Tutorial300'] = np.nan\n",
    "    tutorials = tutorials[['pid', 'Tutorial100', 'Tutorial101', 'Tutorial102', 'Tutorial103',\n",
    "                                  'Tutorial104', 'Tutorial105', 'Tutorial106', 'Tutorial107',\n",
    "                                  'Tutorial108', 'Tutorial109', 'Tutorial110', 'Tutorial200',\n",
    "                                  'Tutorial201', 'Tutorial202', 'Tutorial203', 'Tutorial204',\n",
    "                                  'Tutorial205', 'Tutorial206', 'Tutorial207', 'Tutorial208',\n",
    "                                  'Tutorial209', 'Tutorial300', 'Tutorial301', 'Tutorial302', \n",
    "                                  'Tutorial303', 'Tutorial304']]\n",
    "\n",
    "    ip = df_raw[(df_raw['data_name'] == 'clientId') | (df_raw['data_name'] == 'ipAddress')].reset_index()\n",
    "    ip['clientId'] = np.nan\n",
    "    ip['ipAddress'] = np.nan\n",
    "    for i in range(0, len(ip)-1, 2):\n",
    "        if ip['data_name'][i] == 'clientId':\n",
    "            ip['clientId'][i] = ip['data_value'][i]\n",
    "            ip['ipAddress'][i] = ip['data_value'][i+1]\n",
    "        else:\n",
    "            ip['clientId'][i] = ip['data_value'][i+1]\n",
    "            ip['ipAddress'][i] = ip['data_value'][i]\n",
    "    ip = ip.dropna().drop_duplicates(subset = 'clientId').drop(\n",
    "        labels = ['index', 'event', 'data_name', 'data_value'], axis = 1).reset_index(drop = True)\n",
    "    ip = ip.drop(['id', 'event_date'], axis=1)\n",
    "\n",
    "    screening = df_raw[df_raw['event'] == 'Screening']\n",
    "    gameParams1 = df_raw[df_raw['event'] == 'GameParameters1']\n",
    "    gameParams2 = df_raw[df_raw['event'] == 'GameParameters2']\n",
    "    words = df_raw[df_raw['event'] == 'Words']\n",
    "    \n",
    "    coopChoiceNeighbors = df_raw[df_raw['event'] == 'CoopChoice']\n",
    "    coopChoice = coopChoiceNeighbors[~coopChoiceNeighbors['data_name'].str.contains('neighbor')]\n",
    "    neighbors = coopChoiceNeighbors[coopChoiceNeighbors['data_name'].str.contains('neighbor')]\n",
    "    neighbors = neighbors.sort_values(by = ['data_name'])\n",
    "    \n",
    "    addChoice = df_raw[df_raw['event'] == 'AddChoice']\n",
    "    cutChoice = df_raw[df_raw['event'] == 'CutChoice']\n",
    "    score2 = df_raw[df_raw['event'] == 'Score2']\n",
    "    score3 = df_raw[df_raw['event'] == 'Score3']\n",
    "    fairScore2 = df_raw[df_raw['event'] == 'FairScore2']\n",
    "    fairScore3 = df_raw[df_raw['event'] == 'FairScore3']\n",
    "    trustScore2 = df_raw[df_raw['event'] == 'TrustScore2']\n",
    "    trustScore3 = df_raw[df_raw['event'] == 'TrustScore3']\n",
    "    relative = df_raw[df_raw['event'] == 'Relative']\n",
    "    whyCoop = df_raw[df_raw['event'] == 'WhyCoop']\n",
    "    demographic = df_raw[df_raw['event'] == 'Demographic'].drop(['event'], axis = 1)\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    df_raw = df_raw.drop(['id', 'event_date'], axis = 1)\n",
    "    \n",
    "    # Combine session metadata\n",
    "    df_session_metadata = pd.concat([\n",
    "        makeRectangular(gameParams1), \n",
    "        makeRectangular(gameParams2),\n",
    "        pd.DataFrame([date],columns = ['date']),\n",
    "        pd.DataFrame([hour],columns = ['hour'])], axis = 1)\n",
    "\n",
    "    # Combine player metadata\n",
    "    if relative.empty:\n",
    "        df_player_metadata = ip.rename(\n",
    "            columns = {'clientId':'pid', 'ipAddress':'ip_to_country'}, inplace = False).merge(\n",
    "            tutorials, on='pid', how='outer').merge(\n",
    "            makeRectangular(screening), on='pid', how='outer').merge(\n",
    "            makeRectangular(words), on='pid', how='outer').merge(\n",
    "            makeRectangular(score2), on='pid', how='outer').merge(\n",
    "            makeRectangular(score3), on='pid', how='outer').merge(\n",
    "            makeRectangular(fairScore2), on='pid', how='outer').merge(\n",
    "            makeRectangular(fairScore3), on='pid', how='outer').merge(\n",
    "            makeRectangular(trustScore2), on='pid', how='outer').merge(\n",
    "            makeRectangular(trustScore3), on='pid', how='outer').merge(\n",
    "            makeRectangular(whyCoop), on='pid', how='outer').merge(\n",
    "            makeRectangular(demographic), on='pid', how='outer')\n",
    "    else:\n",
    "        df_player_metadata = ip.rename(\n",
    "            columns = {'clientId':'pid', 'ipAddress':'ip_to_country'}, inplace = False).merge(\n",
    "            tutorials, on='pid', how='outer').merge(\n",
    "            makeRectangular(screening), on='pid', how='outer').merge(\n",
    "            makeRectangular(words), on='pid', how='outer').merge(\n",
    "            makeRectangular(score2), on='pid', how='outer').merge(\n",
    "            makeRectangular(score3), on='pid', how='outer').merge(\n",
    "            makeRectangular(fairScore2), on='pid', how='outer').merge(\n",
    "            makeRectangular(fairScore3), on='pid', how='outer').merge(\n",
    "            makeRectangular(trustScore2), on='pid', how='outer').merge(\n",
    "            makeRectangular(trustScore3), on='pid', how='outer').merge(\n",
    "            makeRectangular(relative), on='pid', how='outer').merge(\n",
    "            makeRectangular(whyCoop), on='pid', how='outer').merge(\n",
    "            makeRectangular(demographic), on='pid', how='outer')\n",
    "\n",
    "    # Replace IP address with country\n",
    "    for index, row in df_player_metadata.iterrows():\n",
    "        url = 'http://ipinfo.io/' + str(row.ip_to_country) + '?token=XXXXXXXXXXXXXX'\n",
    "        response = requests.get(url)\n",
    "        response_text = re.sub(\"true\", \"True\", response.text)\n",
    "        response_text = re.sub(\"false\", \"False\", response_text)\n",
    "        d = ast.literal_eval(response_text)\n",
    "        if \"country\" in d.keys():\n",
    "            df_player_metadata.loc[index, 'ip_to_country'] = d[\"country\"]\n",
    "        else:\n",
    "            df_player_metadata.loc[index, 'ip_to_country'] = \"unknown\"\n",
    "        time.sleep(.1)\n",
    "\n",
    "    # Combine game data\n",
    "    neighbors2 = neighbors['data_name'].str.split('_', n = 3, expand = True)\n",
    "    neighbors2.columns = ['pid', 'version', 'round', 'neighbor_no']\n",
    "    neighbors2['data_value'] = neighbors['data_value']\n",
    "    neighbors2 = neighbors2[(neighbors2['round'] != '0') | (neighbors2['version'] == '2')].reset_index(drop = True)\n",
    "    \n",
    "    neighbors3 = neighbors2.copy()\n",
    "\n",
    "    pattern1 = re.compile(r\"neighbor_0\")\n",
    "    pattern2 = re.compile(r\"(neighbor_[1-9]{1})|(neighbor_[0-9]{2})\")\n",
    "\n",
    "    for index, row in neighbors3.iterrows():\n",
    "        if pattern1.match(str(row.neighbor_no)):\n",
    "            next_row = index + 1\n",
    "            while next_row < len(neighbors3) and pattern2.match(str(neighbors3.iloc[next_row].neighbor_no)):\n",
    "                neighbors3.iloc[index].data_value = \\\n",
    "                    neighbors3.iloc[index].data_value + \", \" + str(neighbors3.iloc[next_row].data_value)\n",
    "                next_row += 1\n",
    "\n",
    "    func = np.vectorize(lambda x: bool(pattern2.match(x)))\n",
    "\n",
    "    neighbors3 = neighbors3[~func(neighbors3.neighbor_no)].replace(\"neighbor_0\", \"neighbors\")\n",
    "    neighbors3 = neighbors3.drop(columns = ['neighbor_no'])\n",
    "    neighbors3 = neighbors3.rename(columns = {\"data_value\": \"neighbors\"})\n",
    "    neighbors3['neighbors'] = \"[\" + neighbors3['neighbors'] + \"]\"\n",
    "    \n",
    "    df_game_data = makeRectangular(coopChoice).merge(\n",
    "        makeRectangular(addChoice), on=['pid', 'round', 'version'], how='outer').merge(\n",
    "        makeRectangular(cutChoice), on=['pid', 'round', 'version'], how='outer', suffixes=('_add', '_cut')).merge(\n",
    "        neighbors3, on=['pid', 'round', 'version'], how='outer')\n",
    "\n",
    "    # Combine everything together\n",
    "    df_full = df_game_data.merge(df_player_metadata, on='pid')\n",
    "    df_full = pd.concat([df_full, df_session_metadata], axis = 1)\n",
    "    df_full.date.replace([float('nan')], [df_full.date[0]], inplace=True)\n",
    "    df_full.hour.replace([float('nan')], [df_full.hour[0]], inplace=True)\n",
    "    df_full.earned1.replace([float('nan')], [df_full.earned1[0]], inplace=True)\n",
    "    df_full.equal1.replace([float('nan')], [df_full.equal1[0]], inplace=True)\n",
    "    df_full.earned2.replace([float('nan')], [df_full.earned2[0]], inplace=True)\n",
    "    df_full.equal2.replace([float('nan')], [df_full.equal2[0]], inplace=True)\n",
    "\n",
    "    # Write to csv\n",
    "    df_full.to_csv(os.path.join(out_path, \"processed-\" + str(file)), sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
